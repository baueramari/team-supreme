{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from convokit import Corpus, download\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists at /Users/amaribauer/.convokit/downloads/supreme-corpus\n"
     ]
    }
   ],
   "source": [
    "corpus = Corpus(filename=download('supreme-corpus'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024 cases\n",
      "65.33 percent of cases were decided favorably for the petitioner\n"
     ]
    }
   ],
   "source": [
    "# All cases\n",
    "cases = pd.read_json(path_or_buf='data/cases.jsonl', lines=True)\n",
    "\n",
    "# Cases with clear winners\n",
    "df = cases.loc[cases.loc[:, 'win_side'].isin([1, 0])]\n",
    "\n",
    "# Roberts court cases with clear winners\n",
    "roberts = df.loc[df.loc[:, 'court'] == 'Roberts Court', :]\n",
    "\n",
    "# All utterances\n",
    "all_utts = corpus.get_utterances_dataframe()\n",
    "\n",
    "# Roberts court case utterances\n",
    "roberts_ids = roberts.loc[:, 'id'].unique()\n",
    "utts = all_utts.loc[all_utts.loc[:, 'meta.case_id'].isin(roberts_ids)]\n",
    "\n",
    "# Roberts court cases with clearn winners and utterance data\n",
    "    # Unique case IDs from utts indicate the number of Roberts court cases\n",
    "    # the corpus has utterance data for\n",
    "subset_ids = utts.loc[:, 'meta.case_id'].unique()\n",
    "    # Use these ids to subset the roberts data frame\n",
    "    # (exclude cases without utterance data)\n",
    "roberts = roberts.loc[roberts.loc[:, 'id'].isin(subset_ids)]\n",
    "print(len(roberts), 'cases')\n",
    "petitioner_wins = roberts.loc[:, 'win_side'].mean()\n",
    "print(round(petitioner_wins * 100, 2),\n",
    "      'percent of cases were decided favorably for the petitioner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 1\n",
      "Max: 1235\n",
      "Mean 237.3955078125\n"
     ]
    }
   ],
   "source": [
    "# Utterances per case\n",
    "print('Min:', utts.groupby(['meta.case_id']).size().min()) \n",
    "print('Max:', utts.groupby(['meta.case_id']).size().max())\n",
    "print('Mean', utts.groupby(['meta.case_id']).size().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 case dropped\n",
      "New min: 66\n",
      "1023 cases\n"
     ]
    }
   ],
   "source": [
    "# Drop single-utterance cases\n",
    "utt_counts = pd.DataFrame(utts.groupby(['meta.case_id']).size())\n",
    "utt_counts = utt_counts.reset_index()\n",
    "utt_counts = utt_counts.rename(columns={0: 'utt_counts'})\n",
    "utts = pd.merge(utts, utt_counts, how = 'left')\n",
    "utts = utts.loc[utts.loc[:, 'utt_counts'] != 1, :]\n",
    "print('1 case dropped')\n",
    "print('New min:', utts.groupby(['meta.case_id']).size().min()) \n",
    "print(len(utts.groupby(['meta.case_id'])), 'cases')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "utts['text'].replace('\\d+', '', regex=True, inplace = True) #remove numbers, decide as group if this makes sense\n",
    "group_utts = utts.groupby('meta.case_id')['text'].apply(' '.join)\n",
    "#NOTE: No stemming or lemmatization done at this point\n",
    "df = pd.merge(group_utts,roberts[['id','win_side']], how = 'left', \n",
    "              left_on = 'meta.case_id', right_on = 'id')\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.loc[:,df.columns != 'win_side'],df['win_side'], test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Vectorizer and vectorize train data\n",
    "count_vect = CountVectorizer(stop_words = 'english') #play around with ngrams, max/min df args\n",
    "count_df = count_vect.fit_transform(X_train['text'])\n",
    "count_array = count_df.toarray()\n",
    "count_df = pd.DataFrame(count_array,columns = count_vect.get_feature_names(), index = X_train['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorize test data\n",
    "count_test = count_vect.transform(X_test['text'])\n",
    "count_test_array = count_test.toarray()\n",
    "count_test_df = pd.DataFrame(count_test_array, columns = count_vect.get_feature_names(),index = X_test['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "COUNT_TEST_DF AND COUNT_DF ARE OUR CURRENT BAG OF WORDS FEATURE SETS. I WILL UPDATE THEM AFTER IMPLEMENTING TF-IDF\n",
    "TO TAKE INTO ACCOUNT CASE LENGTH\n",
    "\"\"\"\n",
    "count_df.to_csv('data/train_unigram_BoW.csv')\n",
    "count_test_df.to_csv('data/test_unigram_BoW.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
