{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from convokit import Corpus, download\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists at /Users/amaribauer/.convokit/downloads/supreme-corpus\n"
     ]
    }
   ],
   "source": [
    "corpus = Corpus(filename=download('supreme-corpus'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024 cases\n",
      "65.33 percent of cases were decided favorably for the petitioner\n"
     ]
    }
   ],
   "source": [
    "# All cases\n",
    "cases = pd.read_json(path_or_buf='data/cases.jsonl', lines=True)\n",
    "\n",
    "# Cases with clear winners\n",
    "df = cases.loc[cases.loc[:, 'win_side'].isin([1, 0])]\n",
    "\n",
    "# Roberts court cases with clear winners\n",
    "roberts = df.loc[df.loc[:, 'court'] == 'Roberts Court', :]\n",
    "\n",
    "# All utterances\n",
    "all_utts = corpus.get_utterances_dataframe()\n",
    "\n",
    "# Roberts court case utterances\n",
    "roberts_ids = roberts.loc[:, 'id'].unique()\n",
    "utts = all_utts.loc[all_utts.loc[:, 'meta.case_id'].isin(roberts_ids)]\n",
    "\n",
    "# Roberts court cases with clearn winners and utterance data\n",
    "    # Unique case IDs from utts indicate the number of Roberts court cases\n",
    "    # the corpus has utterance data for\n",
    "subset_ids = utts.loc[:, 'meta.case_id'].unique()\n",
    "    # Use these ids to subset the roberts data frame\n",
    "    # (exclude cases without utterance data)\n",
    "roberts = roberts.loc[roberts.loc[:, 'id'].isin(subset_ids)]\n",
    "print(len(roberts), 'cases')\n",
    "petitioner_wins = roberts.loc[:, 'win_side'].mean()\n",
    "print(round(petitioner_wins * 100, 2),\n",
    "      'percent of cases were decided favorably for the petitioner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 1\n",
      "Max: 1235\n",
      "Mean 237.3955078125\n"
     ]
    }
   ],
   "source": [
    "# Utterances per case\n",
    "print('Min:', utts.groupby(['meta.case_id']).size().min()) \n",
    "print('Max:', utts.groupby(['meta.case_id']).size().max())\n",
    "print('Mean', utts.groupby(['meta.case_id']).size().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 case dropped\n",
      "New min: 66\n",
      "1023 cases\n"
     ]
    }
   ],
   "source": [
    "# Drop single-utterance cases\n",
    "utt_counts = pd.DataFrame(utts.groupby(['meta.case_id']).size())\n",
    "utt_counts = utt_counts.reset_index()\n",
    "utt_counts = utt_counts.rename(columns={0: 'utt_counts'})\n",
    "utts = pd.merge(utts, utt_counts, how = 'left')\n",
    "utts = utts.loc[utts.loc[:, 'utt_counts'] != 1, :]\n",
    "print('1 case dropped')\n",
    "print('New min:', utts.groupby(['meta.case_id']).size().min()) \n",
    "print(len(utts.groupby(['meta.case_id'])), 'cases')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "utts['text'].replace('\\d+', '', regex=True, inplace = True) #remove numbers, decide as group if this makes sense\n",
    "group_utts = utts.groupby('meta.case_id')['text'].apply(' '.join)\n",
    "#NOTE: No stemming or lemmatization done at this point\n",
    "df = pd.merge(group_utts,roberts[['id','win_side']], how = 'left', \n",
    "              left_on = 'meta.case_id', right_on = 'id')\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.loc[:,df.columns != 'win_side'],df['win_side'], test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaa</th>\n",
       "      <th>aagency</th>\n",
       "      <th>aaidd</th>\n",
       "      <th>aals</th>\n",
       "      <th>aaron</th>\n",
       "      <th>aarp</th>\n",
       "      <th>aba</th>\n",
       "      <th>aback</th>\n",
       "      <th>abacus</th>\n",
       "      <th>abandon</th>\n",
       "      <th>...</th>\n",
       "      <th>zoologist</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zuber</th>\n",
       "      <th>zubik</th>\n",
       "      <th>zubin</th>\n",
       "      <th>zug</th>\n",
       "      <th>zumudio</th>\n",
       "      <th>zuni</th>\n",
       "      <th>zurko</th>\n",
       "      <th>zwickler</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015_15-290</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007505</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007_07-343</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009_09-497</th>\n",
       "      <td>0.006633</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005_04-1131</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009_09-338</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008228</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016_16-6219</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007_06-984</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013_12-79</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012_11-982</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005185</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014_13-628</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>818 rows × 34880 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   aaa  aagency  aaidd  aals  aaron  aarp       aba  aback  \\\n",
       "id                                                                           \n",
       "2015_15-290   0.000000      0.0    0.0   0.0    0.0   0.0  0.000000    0.0   \n",
       "2007_07-343   0.000000      0.0    0.0   0.0    0.0   0.0  0.000000    0.0   \n",
       "2009_09-497   0.006633      0.0    0.0   0.0    0.0   0.0  0.000000    0.0   \n",
       "2005_04-1131  0.000000      0.0    0.0   0.0    0.0   0.0  0.000000    0.0   \n",
       "2009_09-338   0.000000      0.0    0.0   0.0    0.0   0.0  0.008228    0.0   \n",
       "...                ...      ...    ...   ...    ...   ...       ...    ...   \n",
       "2016_16-6219  0.000000      0.0    0.0   0.0    0.0   0.0  0.000000    0.0   \n",
       "2007_06-984   0.000000      0.0    0.0   0.0    0.0   0.0  0.000000    0.0   \n",
       "2013_12-79    0.000000      0.0    0.0   0.0    0.0   0.0  0.000000    0.0   \n",
       "2012_11-982   0.000000      0.0    0.0   0.0    0.0   0.0  0.000000    0.0   \n",
       "2014_13-628   0.000000      0.0    0.0   0.0    0.0   0.0  0.000000    0.0   \n",
       "\n",
       "              abacus   abandon  ...  zoologist  zoom  zuber  zubik  zubin  \\\n",
       "id                              ...                                         \n",
       "2015_15-290      0.0  0.007505  ...        0.0   0.0    0.0    0.0    0.0   \n",
       "2007_07-343      0.0  0.000000  ...        0.0   0.0    0.0    0.0    0.0   \n",
       "2009_09-497      0.0  0.000000  ...        0.0   0.0    0.0    0.0    0.0   \n",
       "2005_04-1131     0.0  0.000000  ...        0.0   0.0    0.0    0.0    0.0   \n",
       "2009_09-338      0.0  0.000000  ...        0.0   0.0    0.0    0.0    0.0   \n",
       "...              ...       ...  ...        ...   ...    ...    ...    ...   \n",
       "2016_16-6219     0.0  0.000000  ...        0.0   0.0    0.0    0.0    0.0   \n",
       "2007_06-984      0.0  0.000000  ...        0.0   0.0    0.0    0.0    0.0   \n",
       "2013_12-79       0.0  0.000000  ...        0.0   0.0    0.0    0.0    0.0   \n",
       "2012_11-982      0.0  0.005185  ...        0.0   0.0    0.0    0.0    0.0   \n",
       "2014_13-628      0.0  0.000000  ...        0.0   0.0    0.0    0.0    0.0   \n",
       "\n",
       "              zug  zumudio  zuni  zurko  zwickler  \n",
       "id                                                 \n",
       "2015_15-290   0.0      0.0   0.0    0.0       0.0  \n",
       "2007_07-343   0.0      0.0   0.0    0.0       0.0  \n",
       "2009_09-497   0.0      0.0   0.0    0.0       0.0  \n",
       "2005_04-1131  0.0      0.0   0.0    0.0       0.0  \n",
       "2009_09-338   0.0      0.0   0.0    0.0       0.0  \n",
       "...           ...      ...   ...    ...       ...  \n",
       "2016_16-6219  0.0      0.0   0.0    0.0       0.0  \n",
       "2007_06-984   0.0      0.0   0.0    0.0       0.0  \n",
       "2013_12-79    0.0      0.0   0.0    0.0       0.0  \n",
       "2012_11-982   0.0      0.0   0.0    0.0       0.0  \n",
       "2014_13-628   0.0      0.0   0.0    0.0       0.0  \n",
       "\n",
       "[818 rows x 34880 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initialize Vectorizer and vectorize train data\n",
    "count_vect = CountVectorizer(ngram_range = (1,1),min_df = 1,\n",
    "            stop_words = 'english', token_pattern = r'\\b[a-zA-Z]{3,}\\b') \n",
    "            #play around with ngrams, max/min df args\n",
    "count_df = count_vect.fit_transform(X_train['text'])\n",
    "count_array = count_df.toarray()\n",
    "count_df = pd.DataFrame(count_array,columns = count_vect.get_feature_names(), index = X_train['id'])\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(count_df).toarray()\n",
    "X_train_tfidf = pd.DataFrame(X_train_tfidf, columns = count_vect.get_feature_names(), index = X_train['id'])\n",
    "X_train_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aaa         0.025469\n",
       "aagency     0.000000\n",
       "aaidd       0.067921\n",
       "aals        0.000000\n",
       "aaron       0.037114\n",
       "              ...   \n",
       "zug         0.000000\n",
       "zumudio     0.000000\n",
       "zuni        0.007864\n",
       "zurko       0.011885\n",
       "zwickler    0.000000\n",
       "Length: 34880, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Vectorize test data\n",
    "count_test = count_vect.transform(X_test['text'])\n",
    "#feature union sklearn function to add in our other features. \n",
    "count_test_array = count_test.toarray()\n",
    "count_test_df = pd.DataFrame(count_test_array, columns = count_vect.get_feature_names(),index = X_test['id'])\n",
    "\n",
    "X_test_tfidf = tfidf_transformer.transform(count_test_df).toarray()\n",
    "X_test_tfidf = pd.DataFrame(X_test_tfidf, columns = count_vect.get_feature_names(), index = X_test['id'])\n",
    "X_test_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "COUNT_TEST_DF AND COUNT_DF ARE OUR CURRENT BAG OF WORDS FEATURE SETS. I WILL UPDATE THEM AFTER IMPLEMENTING TF-IDF\n",
    "TO TAKE INTO ACCOUNT CASE LENGTH\n",
    "\"\"\"\n",
    "count_df.to_csv('data/train_unigram_BoW.csv')\n",
    "count_test_df.to_csv('data/test_unigram_BoW.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
